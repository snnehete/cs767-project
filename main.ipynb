{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Revenue Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve test data from 2015-2019\n",
    "r = requests.get('https://api.themoviedb.org/3/discover/movie?api_key=9447cd662f95276c1c7a053a8d830e7a&language=en-US&region=US&sort_by=revenue.desc&include_adult=false&include_video=false&release_date.gte=2015-01-01&release_date.lte=2019-12-31&with_release_type=3&vote_count.gte=100')\n",
    "data = r.json()\n",
    "pages = data['total_pages']\n",
    "#create a list of all movie IDs from the release range\n",
    "movies = []\n",
    "for i in range(1,pages+1):\n",
    "    r = requests.get('https://api.themoviedb.org/3/discover/movie?api_key=9447cd662f95276c1c7a053a8d830e7a&language=en-US&region=US&sort_by=revenue.desc&include_adult=false&include_video=false&release_date.gte=2015-01-01&release_date.lte=2019-12-31&with_release_type=3&vote_count.gte=100&page=' + str(i))\n",
    "    data = r.json()\n",
    "    results = len(data['results'])\n",
    "    for j in range(results):\n",
    "        movies.append(data['results'][j]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of dictionaries to convert into a dataframe\n",
    "data_frame = []\n",
    "for i in range(len(movies)):\n",
    "    movie_id = str(movies[i])\n",
    "\n",
    "    #retrieve movie details\n",
    "    r = requests.get('https://api.themoviedb.org/3/movie/' + movie_id + '?api_key=9447cd662f95276c1c7a053a8d830e7a&language=en-US')\n",
    "    data = r.json()\n",
    "    movie_budget = data['budget']\n",
    "    movie_production = np.nan\n",
    "    #only consider the primary production company for the region\n",
    "    if len(data['production_companies']):\n",
    "        movie_production = data['production_companies'][0]['id']\n",
    "    movie_release = data['release_date']\n",
    "    movie_revenue = data['revenue']\n",
    "    movie_runtime = data['runtime']\n",
    "    movie_genre = data['genres'][0]['id']\n",
    "    movie_popularity = data['popularity']\n",
    "    movie_votecount = data['vote_count']\n",
    "    movie_voteavg = data['vote_average']\n",
    "\n",
    "    #retrieve keywords associated with the movie\n",
    "    r = requests.get('https://api.themoviedb.org/3/movie/' + movie_id + '/keywords?api_key=9447cd662f95276c1c7a053a8d830e7a')\n",
    "    keywords = r.json()\n",
    "    keyword = []\n",
    "    #only consider three most relevant keywords\n",
    "    if len(keywords['keywords']) < 2:\n",
    "        count = 0\n",
    "        for i in range(len(keywords['keywords'])):\n",
    "            keyword.append(keywords['keywords'][i]['id'])\n",
    "            count += 1\n",
    "        while count < 2:\n",
    "            keyword.append(np.nan)\n",
    "            count += 1\n",
    "    else:\n",
    "        for i in range(len(keywords['keywords'])):\n",
    "            keyword.append(keywords['keywords'][i]['id'])\n",
    "\n",
    "            \n",
    "    #create dictionary for the movie \n",
    "    movie_dict = {'id': movie_id, 'genre': movie_genre, 'keyword1': keyword[0], 'keyword2': keyword[1], 'budget': movie_budget, 'production': movie_production, 'runtime': movie_runtime, 'popularity': movie_popularity, 'vote_count': movie_votecount, 'vote_average': movie_voteavg, 'revenue': movie_revenue}\n",
    "    data_frame.append(movie_dict)\n",
    "    \n",
    "df = pd.DataFrame(data_frame)\n",
    "#drop null and incorrect values\n",
    "df = df.dropna().loc[df['revenue']>0].loc[df['budget']>0]\n",
    "#normalize values\n",
    "#categorize revenues\n",
    "df['revenue'] = df['revenue'].apply(lambda x: x//1000000)\n",
    "df['budget'] = df['budget'].apply(lambda x: x//1000000)\n",
    "df['revenue'] = pd.cut(df['revenue'], bins=[-1, 2, 15, 40, 95, 250, 10000], labels = ['0', '1', '2', '3', '4', '5'])\n",
    "#save pandas dataframe to csv\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "columns = ['genre', 'keyword1', 'keyword2', 'production']\n",
    "for col in columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "df = pd.get_dummies(df, columns=columns)\n",
    "df.to_csv('data.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bingo APHR with SVC = 28.0%\n",
      "1-Away APHR with SVC = 59%\n"
     ]
    }
   ],
   "source": [
    "#SVM Classification\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "#creating the train-test split\n",
    "y = data['revenue']\n",
    "X = data.drop(['revenue'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 87) \n",
    "\n",
    "#training and testing the SVM\n",
    "svm_model = SVC(kernel='rbf', C=10)\n",
    "svm_model.fit(X_train, y_train) \n",
    "svm_predictions = svm_model.predict(X_test)\n",
    "\n",
    "#model accuracy for X_test   \n",
    "bingo_accuracy = (svm_model.score(X_test, y_test))*100\n",
    "aphr = svm_predictions - y_test.to_numpy()\n",
    "one_away = [1 if abs(x) == 1 or x == 0 else 0 for x in aphr]\n",
    "one_away_accuracy = (sum(one_away)/len(one_away))*100\n",
    "print(\"Bingo APHR with SVC = \" + str(round(bingo_accuracy)) + \"%\")\n",
    "print(\"1-Away APHR with SVC = \" + str(round(one_away_accuracy)) + \"%\")\n",
    "\n",
    "#save results as csv\n",
    "results_dict = {'actual': y_test.to_numpy(), 'prediction': svm_predictions}\n",
    "df = pd.DataFrame(results_dict)\n",
    "df = df.replace([0, 1, 2, 3, 4, 5], ['<2M', '2M-15M', '15M-40M', '40M-95M', '95M-250M', '>250M'])\n",
    "df.to_csv('svm_results.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1/50\n",
      "615/615 [==============================] - 6s 9ms/step - loss: 1.8828 - sparse_categorical_accuracy: 0.1821- loss: 1.8678 - sparse_categorical_accuracy: 0. - 3s 11ms/step - loss: 1.8650 - sparse_ca\n",
      "Epoch 2/50\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 1.8521 - sparse_categorical_accuracy: 0.1886\n",
      "Epoch 3/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.8325 - sparse_categorical_accuracy: 0.1935\n",
      "Epoch 4/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.8414 - sparse_categorical_accuracy: 0.1805\n",
      "Epoch 5/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.8263 - sparse_categorical_accuracy: 0.1870\n",
      "Epoch 6/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.8162 - sparse_categorical_accuracy: 0.1821\n",
      "Epoch 7/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.8175 - sparse_categorical_accuracy: 0.1984\n",
      "Epoch 8/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7940 - sparse_categorical_accuracy: 0.2016\n",
      "Epoch 9/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7874 - sparse_categorical_accuracy: 0.1951\n",
      "Epoch 10/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7870 - sparse_categorical_accuracy: 0.1967\n",
      "Epoch 11/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7744 - sparse_categorical_accuracy: 0.2049\n",
      "Epoch 12/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7641 - sparse_categorical_accuracy: 0.2211\n",
      "Epoch 13/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7790 - sparse_categorical_accuracy: 0.2098\n",
      "Epoch 14/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7655 - sparse_categorical_accuracy: 0.1886\n",
      "Epoch 15/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7770 - sparse_categorical_accuracy: 0.2033\n",
      "Epoch 16/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7763 - sparse_categorical_accuracy: 0.1919\n",
      "Epoch 17/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7643 - sparse_categorical_accuracy: 0.2033\n",
      "Epoch 18/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7734 - sparse_categorical_accuracy: 0.2016\n",
      "Epoch 19/50\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 1.7691 - sparse_categorical_accuracy: 0.2033\n",
      "Epoch 20/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7675 - sparse_categorical_accuracy: 0.1756\n",
      "Epoch 21/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7628 - sparse_categorical_accuracy: 0.2081\n",
      "Epoch 22/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7619 - sparse_categorical_accuracy: 0.2049- ETA: 1s - loss: 1.7255 - sparse_categoric\n",
      "Epoch 23/50\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 1.7681 - sparse_categorical_accuracy: 0.1919\n",
      "Epoch 24/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7585 - sparse_categorical_accuracy: 0.1821\n",
      "Epoch 25/50\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 1.7559 - sparse_categorical_accuracy: 0.2195\n",
      "Epoch 26/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7555 - sparse_categorical_accuracy: 0.2081\n",
      "Epoch 27/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7625 - sparse_categorical_accuracy: 0.2000\n",
      "Epoch 28/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7585 - sparse_categorical_accuracy: 0.1805\n",
      "Epoch 29/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7574 - sparse_categorical_accuracy: 0.2000- ETA: 1s - loss: 1.7525 - sparse_categorical_\n",
      "Epoch 30/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7581 - sparse_categorical_accuracy: 0.1967\n",
      "Epoch 31/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7568 - sparse_categorical_accuracy: 0.1902\n",
      "Epoch 32/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7497 - sparse_categorical_accuracy: 0.1984\n",
      "Epoch 33/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7516 - sparse_categorical_accuracy: 0.1886\n",
      "Epoch 34/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7594 - sparse_categorical_accuracy: 0.1902\n",
      "Epoch 35/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7646 - sparse_categorical_accuracy: 0.2065\n",
      "Epoch 36/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7599 - sparse_categorical_accuracy: 0.1967\n",
      "Epoch 37/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7540 - sparse_categorical_accuracy: 0.2179\n",
      "Epoch 38/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7534 - sparse_categorical_accuracy: 0.1902\n",
      "Epoch 39/50\n",
      "615/615 [==============================] - 2s 3ms/step - loss: 1.7591 - sparse_categorical_accuracy: 0.1854\n",
      "Epoch 40/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7618 - sparse_categorical_accuracy: 0.2049- ETA: 1s - loss: 1.7765 - sparse_categori\n",
      "Epoch 41/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7624 - sparse_categorical_accuracy: 0.1902\n",
      "Epoch 42/50\n",
      "615/615 [==============================] - 1s 2ms/step - loss: 1.7545 - sparse_categorical_accuracy: 0.1919\n",
      "Bingo APHR with DNN = 19%\n",
      "1-Away APHR with DNN = 65%\n"
     ]
    }
   ],
   "source": [
    "#DNN classification\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "#manually creating the train-test split\n",
    "test = data.iloc[:100, :]\n",
    "train = data.iloc[100:, :]\n",
    "\n",
    "#converting the pandas dataframe to a tensorflow dataset\n",
    "y_train = train.pop('revenue')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train.values, y_train.values))\n",
    "train_dataset = dataset.shuffle(len(train)).batch(1)\n",
    "y_test = test.pop('revenue')\n",
    "data = tf.data.Dataset.from_tensor_slices((test.values, y_test.values))\n",
    "test_dataset = data.shuffle(len(test)).batch(1)\n",
    "\n",
    "#defining the model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(32, activation='tanh'),\n",
    "  tf.keras.layers.Dense(64, activation='tanh'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(32, activation='tanh'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(6)\n",
    "])\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['sparse_categorical_accuracy'])\n",
    "early_stopping_monitor = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "#train the model and evaluate the accuracy\n",
    "model.fit(train_dataset, epochs=50, callbacks=[early_stopping_monitor])\n",
    "predict = model.predict(test_dataset)\n",
    "\n",
    "#model accuracy for X_test \n",
    "y_test = []\n",
    "for feat, targ in test_dataset:\n",
    "    i = int(targ.numpy())\n",
    "    y_test.append(i)\n",
    "y_test = np.array(y_test)\n",
    "dnn_predictions = np.argmax(predict, axis=1)\n",
    "aphr = dnn_predictions - y_test\n",
    "bingo = [1 if x == 0 else 0 for x in aphr]\n",
    "bingo_accuracy = (sum(bingo)/len(bingo))*100\n",
    "one_away = [1 if abs(x) == 1 or x == 0 else 0 for x in aphr]\n",
    "one_away_accuracy = (sum(one_away)/len(one_away))*100\n",
    "print(\"Bingo APHR with DNN = \" + str(round(bingo_accuracy)) + \"%\")\n",
    "print(\"1-Away APHR with DNN = \" + str(round(one_away_accuracy)) + \"%\")\n",
    "\n",
    "#save results as csv\n",
    "results_dict = {'actual': y_test, 'prediction': dnn_predictions}\n",
    "df = pd.DataFrame(results_dict)\n",
    "df = df.replace([0, 1, 2, 3, 4, 5], ['<2M', '2M-15M', '15M-40M', '40M-95M', '95M-250M', '>250M'])\n",
    "df.to_csv('dnn_results.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
